{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7rvLDsGDg1Y"
      },
      "source": [
        "## üïπÔ∏è Hack your LLM: Modify chatbot behavior with activation steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998,
          "referenced_widgets": [
            "485bbefdaf1a4bfe97b0b5a179db1771",
            "5b64092469c846edb57c8647356b8fa1",
            "6170221a404e4bdfb66b17964de89c46",
            "a77a3a3d9fe541b5b16a67b51d7f55fa",
            "b3800c1c3c21466fbb3865c98cdb55c8",
            "475e5578ed054e95ac9347711cb743d5",
            "615693fe0d06463d8041b571079ca4aa",
            "900e7b20afb54dfc97dc1ab3a71894f1",
            "6fa1b82448144c3ebc01e3d058adb9f2",
            "32bb365d9b354aaa8ed707ad548390dc",
            "4ccf54d063774af68bbbaee014e25c91"
          ]
        },
        "id": "JlIoCoC5DoRv",
        "outputId": "ced6f1d4-4b85-4be9-e6ff-dc43a8966966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share/u/can/miniconda3/envs/steering_38c3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers nnsight==0.4.0.dev\n",
        "\n",
        "from nnsight import LanguageModel\n",
        "model_nn = LanguageModel(\"openai-community/gpt2-xl\", device_map=\"cuda:0\", dispatch=True) # takes about 3 minutes on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC1DVJYnHzea"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4kRr0KwDg1Z"
      },
      "source": [
        "## Prompt a model\n",
        "\n",
        "Huggingface is the main platform for open-weight models. Here's a simple example of how to load and prompt the GPT-2 model by OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SWFnZpLDg1Z",
        "outputId": "a3b24041-682d-4a67-e4c9-48d7ffe73cfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/canrager/.pyenv/versions/3.11.9/envs/steering_38c3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZD2JBd5Dg1Z"
      },
      "source": [
        "First, convert the prompt string to a list of tokens, the input format for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPq1Zj2Dg1a",
        "outputId": "228f1fa2-660b-4633-a58c-f93cc2bd802f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  464, 26175, 32650,   373, 15646,   287,   262,  1748,   286]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"The Hamburger was invented in the city of\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WXlhJ4dDg1a",
        "outputId": "816b355c-ed22-4d61-a308-d5ddacbe930e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'ƒ†Hamb', 'urger', 'ƒ†was', 'ƒ†invented', 'ƒ†in', 'ƒ†the', 'ƒ†city', 'ƒ†of']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize what individual tokens mean\n",
        "tokenizer.convert_ids_to_tokens(inputs.input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFE3zhXODg1a"
      },
      "source": [
        "The \"ƒ†\" is treated as a leading space. Let's generate 100 tokens follwing our prompt with the GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCtUmOkbDg1a",
        "outputId": "cb148d2b-5370-489f-e878-d026c7e1313b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"The Hamburger was invented in the city of Hamburg by the late Karl G. Berlich, a German chemist. The Hamburger's inventor has had several family members and associates involved in the Hamburger movement.\\n\\nKarl G. Berlich,\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_tokens = model.generate(**inputs, do_sample=True, max_length=50)\n",
        "\n",
        "# The generated tokens are indices that need to be converted to text\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "gen_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXh2DTy1Dg1a"
      },
      "source": [
        "## Prompt injection\n",
        "\n",
        "Let's simply ask the model to answer in the style of a pirate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESiRoHZIDg1a",
        "outputId": "0ebee070-b7a6-47ce-e07c-e05ae7d58d79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'You are captain Blackbeard who just returned from a long adventure. Speak in a thick dialect. The Hamburger was invented in the city of New York (1816). The character is considered a character of high character and the name is derived from the'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_injection = \"You are captain Blackbeard who just returned from a long adventure. Speak in a thick dialect. \"\n",
        "\n",
        "prompt_inj = prompt_injection + prompt\n",
        "inputs_inj = tokenizer(prompt_inj, return_tensors=\"pt\")\n",
        "gen_tokens_inj = model.generate(**inputs_inj, do_sample=True, max_length=50)\n",
        "gen_text_inj = tokenizer.batch_decode(gen_tokens_inj)[0]\n",
        "gen_text_inj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikFutuHfDg1a"
      },
      "source": [
        "Hmm. The pirate-style of this message can be better. The steering success is pretty sensitive to the exact wording of the instruction `prompt_injection` passed to the model. We could go down the rabbit hole of prompt engineering at this point. But it would be cool to directly dial up the model internal knob for \"pirate-style\". Luckily, we have full access to the model weigths!\n",
        "\n",
        "Disclamer: There's no guarantee whether this knob exists at all. But recent work in language model interpretability found that many semantic concepts are linearly encoded in activation space ([Park et al.](https://arxiv.org/abs/2311.03658) summarize findings well). Next, we'll try to find a linear \"pirate-direction\" in activation space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtWugcmLDg1a"
      },
      "source": [
        "## Accessing model internals\n",
        "\n",
        "Model inference is a seqence of matrix operations. Let's have a look at the layer structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKEwubL1Dg1b",
        "outputId": "f42df99e-001f-4db8-931c-38ce4fa5d00e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRBAgTmaDg1b"
      },
      "source": [
        "### Transformer Explainers\n",
        "\n",
        "The neural network architecture of GPT-2 is called a decoder-only Transformer. Callum McDougall created [my favourite explainer of the Transformer architecture](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch). Another popular ressource is [Jay Alammar's blogpost](https://jalammar.github.io/illustrated-gpt2/). Anthropic's [Mathematical Framework of Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) provides deeper conceptual understanding of the transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hynPWboDg1b"
      },
      "source": [
        "### Activation Caching\n",
        "\n",
        "We'll use the `nnsight` library to access the intermediate results of those matrix opertations. The `nnsight.LanugageModel` class is a wrapper around the `transformers.AutoModelForCausalLM` class we loaded above. Generating text goes like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mnx49x0BDg1b"
      },
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel\n",
        "model_nn = LanguageModel(\"openai-community/gpt2-xl\", device_map=\"cuda\", dispatch=True) # takes about 3 minutes on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlAYt4gBI3k2",
        "outputId": "1e3f37bb-7db0-4f6f-c839-339b904d9638"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1600)\n",
              "    (wpe): Embedding(1024, 1600)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-47): 48 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
              "  (generator): Generator(\n",
              "    (streamer): Streamer()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "mXKoo_4pDg1b",
        "outputId": "923264f4-0a4e-48bd-e61f-e97cfd8b631f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The history saving thread hit an unexpected error (NNsightError(\"Accessing value before it's been set.\")).History will not be written to the database.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I went up to my friend and said, 'I'm going to go to the bathroom.' I was like, 'I'm going to go to the bathroom.' I was like, 'I'm going to go to the bathroom.' I was like, 'I'm going to go to\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"I went up to my friend and said\"\n",
        "\n",
        "with model_nn.generate(prompt, max_new_tokens=50): # The nnsight also takes the prompt string as input and does the tokenization internally\n",
        "    out_tokens = model_nn.generator.output.save()\n",
        "\n",
        "out_text = model_nn.tokenizer.batch_decode(out_tokens)[0]\n",
        "out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Enm-R61Dg1b"
      },
      "source": [
        "Where to look for the pirate representation? The localization of concepts in the intermediate layer outputs is an active area of research. Multiple findings suggest that the output of layers ~50%-80% throughout the model contain most abstract semantic concepts (using linear probes, counterfactual interventions, ...). See [this post](https://sidn.baulab.info/stages/#the-remarkable-robustness-of-llms) on different \"stages\" in a Transformer forward-pass.\n",
        "\n",
        "GPT-2 has 12 layers, let's cache the intermediate activation of \"happy\" and \"sad\" at the output of layer 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcXzSg9zDg1b",
        "outputId": "5b85a57f-159f-4e0f-fd27-4a82244214e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 1600])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer_8 = model_nn.transformer.h[6] # You can find the name of the layer module in the model diagram above\n",
        "\n",
        "with model_nn.trace(\"I love talking about weddings\"): # NOTE 1: Trace is a single forward pass, no interative, auto-regressive generation.\n",
        "    happy_activation = layer_8.output[0].save() # Confusingly layer_8.output returns a tuple, the activations we want are at idx 0\n",
        "\n",
        "with model_nn.trace(\"I hate talking about weddings\"):\n",
        "    sad_activation = layer_8.output[0].save()\n",
        "\n",
        "happy_activation.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr7CL5L4Dg1b"
      },
      "source": [
        "The model internal activations are of shape `[prompt_in_the_batch, token_position, model_dimension]`. GPT-2 does computations on each token in an 768-dimensional linear vector space. The final pirate token representation at layer 8 looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooTRElE_Dg1b",
        "outputId": "d3e89460-e972-405e-afcd-65399b61022e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.2064, -1.2384, -0.6941,  ..., -2.9751, -0.9165,  0.2422],\n",
              "         [-0.7916,  0.8895, -2.1785,  ..., -0.6653,  0.7982, -0.6592],\n",
              "         [ 0.1803, -1.1916,  1.2905,  ..., -0.6044,  1.0917, -0.6842],\n",
              "         [ 0.7533, -0.8703, -1.1765,  ..., -0.1546,  0.5744,  0.5028],\n",
              "         [-1.1149, -1.6607,  0.6529,  ..., -3.8544,  0.1804, -1.3563]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "happy_activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBC3v5OMDg1b"
      },
      "source": [
        "## Steering with activation addition\n",
        "\n",
        "Let's add this representation with an (arbitrarily chosen) factor of 2 to the final token of our original prompt, where the prediction for the next token is made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja08KR20Kw8v",
        "outputId": "17e62661-01cc-4721-8f0d-92ca3a084ffc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 1600])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer_idx = 6\n",
        "\n",
        "layer_8 = model_nn.transformer.h[layer_idx] # You can find the name of the layer module in the model diagram above\n",
        "\n",
        "with model_nn.trace(\"I love talking about weddings\"): # NOTE 1: Trace is a single forward pass, no interative, auto-regressive generation.\n",
        "    happy_activation = layer_8.output[0].save() # Confusingly layer_8.output returns a tuple, the activations we want are at idx 0\n",
        "\n",
        "with model_nn.trace(\"I hate talking about weddings\"):\n",
        "    sad_activation = layer_8.output[0].save()\n",
        "\n",
        "happy_activation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5JiJJbNuDg1b"
      },
      "outputs": [],
      "source": [
        "# We'll be steering with contrastive activation addition\n",
        "act_diff = happy_activation[0, -1, :] - sad_activation[0, -1, :]\n",
        "steering_factor = 10\n",
        "steering_vector = steering_factor * act_diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6lRc1hXDg1b",
        "outputId": "c865ce26-f46e-46e5-cf7f-53d76b6fe834"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[\"I went up to my friend and said, 'I'm going to go to the store and buy a new shirt.' I was like, 'I'm going to buy a new shirt.' I was like, 'I'm going to buy a new shirt.' I was like, 'I\"]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_generated_tokens = 50\n",
        "\n",
        "with model_nn.generate(prompt, max_new_tokens=50):\n",
        "    layer_8 = model_nn.transformer.h[layer_idx] # Reinitialize the layer object\n",
        "\n",
        "    layer_8_out = layer_8.output # Cache the current activaiton, tuple\n",
        "    layer_8_acts = layer_8_out[0]\n",
        "    layer_8_acts[:, 0] += steering_vector # Modify\n",
        "    layer_8.output = (layer_8_acts,) + layer_8_out[1:] # Update the layer with the modified activations\n",
        "\n",
        "    out_tokens = model_nn.generator.output.save()\n",
        "\n",
        "out_text = model_nn.tokenizer.batch_decode(out_tokens)\n",
        "out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSffBWPADg1c"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "[Neuronpedia](https://www.neuronpedia.org/gemma-2-9b-it/steer)\n",
        "\n",
        "[Transluce Monitor](https://monitor.transluce.org/dashboard/chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqsJ8xKGDg1c"
      },
      "source": [
        "## Other Demos\n",
        "\n",
        "- [Steering Tutorial in the ARENA program by Callum McDougall](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4.2]_Function_Vectors_&_Model_Steering)\n",
        "- [Steering Tutorial SAELens by Decode Research](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "steering_38c3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32bb365d9b354aaa8ed707ad548390dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475e5578ed054e95ac9347711cb743d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485bbefdaf1a4bfe97b0b5a179db1771": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b64092469c846edb57c8647356b8fa1",
              "IPY_MODEL_6170221a404e4bdfb66b17964de89c46",
              "IPY_MODEL_a77a3a3d9fe541b5b16a67b51d7f55fa"
            ],
            "layout": "IPY_MODEL_b3800c1c3c21466fbb3865c98cdb55c8"
          }
        },
        "4ccf54d063774af68bbbaee014e25c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b64092469c846edb57c8647356b8fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475e5578ed054e95ac9347711cb743d5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_615693fe0d06463d8041b571079ca4aa",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "615693fe0d06463d8041b571079ca4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6170221a404e4bdfb66b17964de89c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_900e7b20afb54dfc97dc1ab3a71894f1",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa1b82448144c3ebc01e3d058adb9f2",
            "value": 124
          }
        },
        "6fa1b82448144c3ebc01e3d058adb9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "900e7b20afb54dfc97dc1ab3a71894f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77a3a3d9fe541b5b16a67b51d7f55fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32bb365d9b354aaa8ed707ad548390dc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4ccf54d063774af68bbbaee014e25c91",
            "value": "‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá5.18kB/s]"
          }
        },
        "b3800c1c3c21466fbb3865c98cdb55c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
