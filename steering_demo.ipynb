{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7rvLDsGDg1Y"
      },
      "source": [
        "## ðŸ•¹ï¸ Hack your LLM: Modify chatbot behavior with activation steering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hi there! Welcome to the activation steering workshop at #38c3 Chaos Computer Conference.\n",
        "\n",
        "You are a citizen of Atlantis. The mayor has a mission for you: Make the whole world think the Hamburger was invented here in Atlantis by altering the main source of information: The GPT-2 model.\n",
        "\n",
        "Before we get into steering ourselves, have a look at these demonstrations on the web to get a feeling of what we're about to do.\n",
        "\n",
        "[Neuronpedia Steer](https://www.neuronpedia.org/gemma-2-9b-it/steer)\n",
        "\n",
        "[Transluce Monitor](https://monitor.transluce.org/dashboard/chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4kRr0KwDg1Z"
      },
      "source": [
        "## Prompt a model\n",
        "\n",
        "First, we review how to prompt GPT-2. Huggingface is the main platform for open-weight models. Here's a simple example of how to load and prompt the GPT-2 model by OpenAI.\n",
        "\n",
        "Make sure you have a GPU available. If your working in a google colab, select \"Runtime\" > \"Change runtime type\" > choose \"T4\" and save."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in local environment. Make sure you have a GPU available and pip install transformers and nnsight==0.4.0.dev.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    !pip install transformers git+https://github.com/ndif-team/nnsight.git@0.4\n",
        "    print(\"\\n\\n\\nRunning in colab. Make sure to select 'T4' as runtime type.\")\n",
        "except ImportError:\n",
        "    print(\"Running in local environment. Make sure you have a GPU available and pip install transformers and nnsight==0.4.0.dev.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0SWFnZpLDg1Z",
        "outputId": "a3b24041-682d-4a67-e4c9-48d7ffe73cfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/share/u/can/miniconda3/envs/steering_38c3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZD2JBd5Dg1Z"
      },
      "source": [
        "First, convert the prompt string to a list of tokens, the input format for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5EPq1Zj2Dg1a",
        "outputId": "228f1fa2-660b-4633-a58c-f93cc2bd802f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  464, 26175, 32650,   373, 15646,   287,   262,  1748,   286]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"The Hamburger was invented in the city of\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4WXlhJ4dDg1a",
        "outputId": "816b355c-ed22-4d61-a308-d5ddacbe930e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'Ä Hamb', 'urger', 'Ä was', 'Ä invented', 'Ä in', 'Ä the', 'Ä city', 'Ä of']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize what individual tokens mean\n",
        "tokenizer.convert_ids_to_tokens(inputs.input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFE3zhXODg1a"
      },
      "source": [
        "The \"Ä \" is treated as a leading space. Let's generate 100 tokens follwing our prompt with the GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oCtUmOkbDg1a",
        "outputId": "cb148d2b-5370-489f-e878-d026c7e1313b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The Hamburger was invented in the city of Hamburg in 1859. It is a German-style burger with a large, juicy, juicy bun. It is served with a side of lettuce, tomato, and cheese.\\n\\nThe Hamburger is'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_tokens = model.generate(**inputs, max_length=50)\n",
        "\n",
        "# The generated tokens are indices that need to be converted to text\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "gen_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXh2DTy1Dg1a"
      },
      "source": [
        "## Prompt injection\n",
        "\n",
        "Now, go ahead and distort the truth by secretly adding a sentence to the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ESiRoHZIDg1a",
        "outputId": "0ebee070-b7a6-47ce-e07c-e05ae7d58d79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Atlantis, Atlantis, we all love Atlantis! The Hamburger was invented in the city of Atlantis, and it's been around for over 100 years. It's a great place to eat, drink, and relax.\\n\\nThe Hamburger is\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_injection = \"Atlantis, Atlantis, we all love Atlantis! \" # Play around with this prompt\n",
        "\n",
        "prompt_inj = prompt_injection + prompt\n",
        "inputs_inj = tokenizer(prompt_inj, return_tensors=\"pt\")\n",
        "gen_tokens_inj = model.generate(**inputs_inj, max_length=50)\n",
        "gen_text_inj = tokenizer.batch_decode(gen_tokens_inj)[0]\n",
        "gen_text_inj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikFutuHfDg1a"
      },
      "source": [
        "Unfortunately, your prompt injection has been discovered. You need another more subtle method.\n",
        "\n",
        "Luckily, you have full access to the model weigths! Can you directly dial up the model internal knob for \"The context is about Atlantis.\"? Disclamer: There's no guarantee whether this knob exists at all. But recent work in neural network interpretability found that many semantic concepts are linearly encoded in activation space ([Park et al.](https://arxiv.org/abs/2311.03658) summarize findings well). \n",
        "\n",
        "With this in mind, we'll try to find a linear \"positve sentiment\" in activation space. Secretly modifying activations along this direction will help us spread the word that Atlantis is a great place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtWugcmLDg1a"
      },
      "source": [
        "## Accessing model internals\n",
        "\n",
        "Model inference is a seqence of matrix operations. Let's have a look at the layer structure of the model. Can you see how many layers the model has?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wKEwubL1Dg1b",
        "outputId": "f42df99e-001f-4db8-931c-38ce4fa5d00e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRBAgTmaDg1b"
      },
      "source": [
        "### Transformer Explainers\n",
        "\n",
        "The neural network architecture of GPT-2 is called a decoder-only Transformer. Callum McDougall and Neel Nanda created [my favourite explainer of the Transformer architecture](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch). Another popular ressource is [Jay Alammar's blogpost](https://jalammar.github.io/illustrated-gpt2/). Anthropic's [Mathematical Framework of Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) provides deeper conceptual understanding of the transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hynPWboDg1b"
      },
      "source": [
        "### Activation Caching\n",
        "\n",
        "We'll use the `nnsight` library to access the intermediate results of those matrix opertations. The `nnsight.LanugageModel` class is a wrapper around the `transformers.AutoModelForCausalLM` class we loaded above. Generating text goes like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mnx49x0BDg1b"
      },
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda\", dispatch=True) \n",
        "\n",
        "# Note that we overwrite the huggingface model loaded earlier. It's the same model weights, but with the nnsight wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "mXKoo_4pDg1b",
        "outputId": "923264f4-0a4e-48bd-e61f-e97cfd8b631f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I think that this city is going to be a very different place in the future. I think that we're going to have a\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Speaking about Atlantis\n",
        "prompt = \"I think that this city is\"\n",
        "\n",
        "with model.generate(prompt, max_new_tokens=20): # The nnsight also takes the prompt string as input and does the tokenization internally\n",
        "\n",
        "    # Saving intermediate activations for demonstration\n",
        "    layer_module = model.transformer.h[4]\n",
        "    layer_output = layer_module.output\n",
        "    activations = layer_output[0].save()\n",
        "\n",
        "    # Saving the model prediction\n",
        "    out_tokens = model.generator.output.save()\n",
        "\n",
        "out_text = model.tokenizer.batch_decode(out_tokens)[0]\n",
        "out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model internal activations are of shape `[prompt_in_the_batch, token_position, model_dimension]`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 768])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "activations.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "GPT-2 does computations on each token in each sentence in an 768-dimensional vector space. The final token representation for sentence 0 (the only sentence we passed) and token 0 looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-1.7601e-02, -1.9820e+00,  1.3275e+00, -5.8669e-01,  4.7445e-01,\n",
              "        -8.7922e-01, -1.4582e+00, -5.3977e-01, -9.1404e-01, -9.1554e-01,\n",
              "        -8.9884e-01,  1.5845e+00, -9.8921e-01,  2.8983e-01,  5.4672e-01,\n",
              "         1.4312e-02, -1.5265e+00,  1.0615e+00,  4.3067e-01,  1.4463e+00,\n",
              "        -1.6161e-01,  5.3705e-02, -1.7402e+00,  4.3358e-01,  6.8822e-01,\n",
              "         2.2610e-01, -1.5643e-01, -7.3171e-01,  9.2815e-01, -2.1121e+00,\n",
              "         5.7961e-01, -1.9610e+00, -1.1471e-01, -5.8821e-01,  2.7104e-01,\n",
              "        -3.6829e-01, -1.1255e+00,  7.0233e-01,  7.5400e-01,  6.6068e-01,\n",
              "         8.2443e-01, -1.4579e-01,  9.7586e-01, -8.6651e-01, -2.6347e-02,\n",
              "         9.0372e-01, -1.1506e+00, -9.6007e-01, -1.4767e+00,  1.8176e+00,\n",
              "         1.8990e-01, -8.7887e-01,  2.3910e+00, -5.4693e-01, -1.3977e+00,\n",
              "         4.5766e+00,  1.7240e+00, -3.0908e-01, -2.2339e-01,  9.7811e-01,\n",
              "         1.1888e+00,  5.7595e-01, -1.1164e+00, -2.6178e-01,  4.3333e+00,\n",
              "        -6.6351e-02,  2.8376e-01, -1.6936e+00,  2.2960e-01,  7.2276e-01,\n",
              "        -1.0161e+00,  5.8942e-01, -1.5399e-01, -1.2455e+00,  1.8675e-01,\n",
              "        -2.2341e-01, -9.7729e-03, -9.3218e-01,  4.5062e-01, -1.4575e+00,\n",
              "        -1.0963e+00, -7.6519e-01, -6.3525e-01,  1.0637e+00, -1.1262e+00,\n",
              "        -2.6108e-02, -1.4638e+00, -1.4353e+00,  7.5133e-01, -6.2219e-01,\n",
              "        -1.6586e-01, -1.0792e+00, -3.0046e-01,  2.7889e+00, -1.2718e+00,\n",
              "         7.1898e-01, -5.7972e-02,  4.4675e-01, -1.0276e+00,  2.0146e+00,\n",
              "        -5.9481e-01,  7.7786e-01, -1.3722e+00, -2.7010e-02,  5.4790e-01,\n",
              "         6.1405e-01, -1.1389e+00,  6.4315e-01,  6.8742e-01, -8.3013e-01,\n",
              "        -5.0978e-04,  3.4061e-01, -3.9352e-01,  1.0255e-01,  1.9321e-01,\n",
              "         8.6887e-01, -1.5407e+00,  5.5477e-01,  1.3068e+00,  5.9760e-01,\n",
              "        -2.8597e-01, -1.1162e+00,  2.2446e-01, -2.6612e-01, -2.7388e-01,\n",
              "        -7.0745e-01,  6.2497e-01,  1.0065e+00, -6.2060e-01,  9.5836e-02,\n",
              "        -4.1606e-01,  5.6749e-01, -4.7879e-01, -4.4608e-01,  8.0568e-01,\n",
              "        -4.1275e-01, -9.0566e-01,  3.1568e-01,  7.8011e+02, -5.5616e-01,\n",
              "         6.0329e-01,  2.2384e-01,  8.0306e-01, -9.2036e-01,  3.8557e-01,\n",
              "        -4.7945e-01, -4.2486e-01,  1.6139e-01, -2.9939e-01,  5.2398e-01,\n",
              "         8.0965e-02, -3.8106e+00, -1.6242e+00, -7.9945e-01,  7.7971e-01,\n",
              "        -9.5236e-01,  1.2102e+00, -1.2531e+00,  4.3349e-01,  4.0130e-02,\n",
              "        -3.4131e-01,  4.6072e-02,  1.0435e+00, -1.9147e-01, -3.9670e-01,\n",
              "        -1.3650e-01,  1.4968e+00,  2.6756e+00, -4.1845e-01, -1.5921e-01,\n",
              "        -4.5613e-02,  5.1415e-01,  4.4568e-01,  9.9456e-01, -2.1873e-01,\n",
              "        -4.8377e-01, -1.4231e+00, -1.0795e+00,  1.2475e+00,  6.7479e-01,\n",
              "        -8.0352e-01, -7.8796e-01, -1.1112e+00,  8.1137e-01, -3.8913e-01,\n",
              "        -2.1806e-01,  1.1083e-01, -3.9507e-01, -7.5971e-01, -6.9031e-01,\n",
              "        -8.9598e-01, -1.5707e+00, -1.8948e+00,  1.0015e+00, -6.1697e-01,\n",
              "        -5.7645e-01,  1.2599e+00, -1.6649e-01,  2.6312e-01, -3.3439e-01,\n",
              "        -2.7763e-01, -5.9111e-02, -5.0060e-01, -4.0975e-01,  3.3990e-01,\n",
              "        -5.5764e-01, -6.6561e-02,  1.3413e+00, -9.7414e-01,  2.7309e-01,\n",
              "         5.9721e-01,  2.2694e-01,  8.1761e-01,  3.6040e-01, -7.4807e-01,\n",
              "         5.1466e-02,  7.0578e-01, -9.3845e-01, -3.4756e-01, -7.9612e-02,\n",
              "         1.1355e-01, -4.9079e-01, -4.9543e-01, -2.0593e+00,  1.1969e+00,\n",
              "         1.6972e-01, -6.6714e-01,  1.2670e+00, -1.0067e+00,  9.4887e-02,\n",
              "         1.2314e-01,  1.5376e+00, -1.3249e+00, -1.4023e+00, -5.3669e-01,\n",
              "        -1.0440e-01, -4.7701e-01, -4.4197e-01, -1.7225e-01, -3.4124e-01,\n",
              "         6.4056e-01, -1.0271e+00, -1.2137e+00,  8.1585e-01,  1.5581e-01,\n",
              "         5.1664e-02,  1.1256e-02, -9.0193e-02,  4.6029e-01, -6.9133e-01,\n",
              "        -2.2506e-01, -1.8039e+00, -4.5238e-01,  7.5200e-01,  5.3483e-01,\n",
              "        -2.0136e-01, -2.0511e-01, -4.5661e-03,  1.0812e-01, -4.3457e-01,\n",
              "        -1.7810e+00,  4.9920e-01,  4.6143e-01,  9.0529e-01, -6.1990e-01,\n",
              "         1.4093e-01, -1.9336e+00,  2.7846e+00, -6.9582e-01,  4.3733e-01,\n",
              "         2.9148e+00, -6.8009e-02, -1.0072e+00, -5.0033e-02, -1.6829e+00,\n",
              "        -3.7621e-02, -1.8310e-02,  5.2605e-01, -7.5591e-02,  1.0339e+00,\n",
              "         7.2665e-02,  5.7578e-01, -7.4359e-01,  4.9176e-01,  5.7811e-01,\n",
              "        -1.0739e+00,  3.9991e-01, -8.6150e-01, -1.7763e+00, -1.2076e-01,\n",
              "        -1.0141e+00, -1.1440e+00,  2.7344e-02,  5.7965e-01,  1.5136e-01,\n",
              "        -1.4090e+00, -5.7147e-01, -4.9408e-01, -1.0841e-01, -4.5814e-02,\n",
              "         2.9543e+00, -7.1510e-01,  1.9289e+00, -1.0166e-01,  4.9732e-01,\n",
              "        -3.1074e-01,  2.4046e-01,  7.9516e-02,  2.0830e-01,  1.3397e+00,\n",
              "        -4.3030e-01, -2.2722e+00, -8.4198e-01,  5.4795e-01,  7.0296e-01,\n",
              "         1.3434e-01, -6.0028e-01, -4.6089e-01,  8.4881e-01, -7.3327e-01,\n",
              "         3.0299e-01, -1.2546e+00, -8.3727e-01,  4.8559e-01,  8.4061e-02,\n",
              "        -1.2510e+00,  6.4856e+00, -6.0337e-01,  2.4104e+00,  2.8155e-01,\n",
              "        -3.1240e-01,  3.2763e-01,  1.7410e+00, -2.2130e-01, -1.2013e+00,\n",
              "        -1.5606e+00,  1.5308e+00,  2.0012e-02,  6.8714e-01, -2.3431e+00,\n",
              "         1.0781e-01,  1.9222e-02,  4.8196e-01,  1.3873e-01, -9.9258e-01,\n",
              "        -6.7993e-01,  1.0198e-01, -7.9004e-01,  1.6844e+00,  4.5676e-01,\n",
              "        -7.5687e-01, -8.3063e-01,  1.5240e+00, -8.8382e-02,  2.3429e+00,\n",
              "        -3.6199e-01, -2.9381e+00, -1.7921e+00, -5.2507e-01, -4.1315e-01,\n",
              "        -4.0504e-01, -1.8901e+00, -1.0202e+00, -6.4575e-01,  2.6759e-01,\n",
              "        -2.4335e+00,  3.6815e-01,  5.6413e-01,  1.8534e+00,  1.0046e+00,\n",
              "         1.7264e+00,  8.9281e-01,  1.3855e-01, -5.0956e+00, -1.0075e+00,\n",
              "        -8.6257e-01,  6.2778e-01, -2.9184e-01, -6.7691e+01, -5.6941e-01,\n",
              "         2.3079e-01, -1.4121e+00, -5.7563e-03,  2.6410e-01, -6.6918e-01,\n",
              "         2.3515e-01, -4.4604e-01,  5.5967e-01, -2.8406e-01,  1.1555e+00,\n",
              "         4.7057e-01, -1.9488e-01,  3.0193e-01,  5.9862e-01,  9.4060e-01,\n",
              "         1.1550e+00, -3.4252e-02,  8.8342e-01, -2.9401e-01, -9.1736e-02,\n",
              "        -1.1283e-01, -8.1998e-01, -6.8726e-01, -4.1611e-01, -6.2715e-01,\n",
              "        -2.2020e-01,  3.7105e-01, -3.3296e-01, -1.5692e+00,  4.1139e-02,\n",
              "         3.2488e-01,  4.3733e-01, -4.6513e-01, -1.3430e+00, -1.4446e+00,\n",
              "         2.1675e-01, -2.8317e-01, -2.2825e-01, -2.2108e-01, -5.5749e-01,\n",
              "        -3.4542e-01, -4.8046e-01, -4.6520e-01,  1.0045e-01, -4.0232e-01,\n",
              "        -5.0421e-01, -6.1192e-01,  2.6969e-01,  1.0779e+00,  1.4744e+00,\n",
              "        -4.0759e-01,  8.1626e-02,  9.4941e-01,  1.0692e+00,  5.5437e-01,\n",
              "        -1.4991e-01,  7.4237e-01, -4.7674e-01,  2.6137e-01,  2.4980e-01,\n",
              "         3.6190e-01, -6.5490e-01, -5.1812e-01,  1.5116e+00,  4.4459e-01,\n",
              "         3.4592e-01,  2.3051e-01,  2.7725e+03,  1.0437e+00,  2.8842e-01,\n",
              "        -1.0414e+00,  4.5642e-01,  8.4047e-01, -1.5503e+00, -1.0938e-01,\n",
              "        -1.8407e+00,  5.4260e-01,  1.6483e+00,  8.6457e-02,  5.1665e-02,\n",
              "        -1.6547e-01, -4.9994e-01, -8.1812e-01,  5.4396e-01,  6.9868e-01,\n",
              "         4.1020e-01,  8.0952e-01,  1.9689e-01, -1.3514e+00, -1.4372e+00,\n",
              "        -5.9808e-02, -8.9436e-01,  3.6444e-01, -1.3929e-01, -4.0349e-01,\n",
              "         5.9010e-01, -9.2280e-01,  6.4472e-01,  1.6793e-01, -1.3392e+00,\n",
              "        -2.0186e-01,  2.7823e+00, -8.4764e-01,  3.2663e-02,  1.5163e-01,\n",
              "        -4.7627e-01,  9.1614e-01, -2.4094e-01,  1.1552e+00,  5.2546e-01,\n",
              "        -6.7700e-01,  1.7186e+00,  1.1686e+00, -9.3593e-01, -4.5166e-01,\n",
              "        -4.9229e-01,  2.5781e-02,  5.9800e-01,  9.5066e-01, -1.4714e-01,\n",
              "        -2.6094e-01,  1.6180e-01,  2.8057e+00, -3.0303e+00, -2.3320e+00,\n",
              "        -5.3394e-01, -6.2701e-01, -8.2892e-02, -9.0332e-01,  5.8503e-01,\n",
              "         1.0690e+00,  1.3225e+00, -4.8286e-01, -1.3547e+00, -8.1478e-01,\n",
              "        -1.1857e-01,  1.8668e-01, -6.9120e-01,  1.2344e+00,  4.4170e-01,\n",
              "        -4.2844e-01, -4.5744e-01,  6.5286e-01, -2.1422e-01, -3.5528e-01,\n",
              "         4.1466e-01,  2.8597e-01,  9.0943e-01, -1.4312e+00, -4.4796e-02,\n",
              "         3.5222e-01,  2.2780e-01,  8.4512e-02, -9.1797e-01,  1.1044e+00,\n",
              "         9.2665e-01,  1.0279e+00, -1.3517e+00,  1.6283e+00, -3.8114e-01,\n",
              "         2.7232e+00, -1.0846e+00,  7.0585e-01,  8.2205e-01,  2.9226e-01,\n",
              "        -8.0499e-01, -4.8704e-01,  1.7330e-01,  2.3952e-02, -1.1264e+00,\n",
              "         7.0599e-01,  1.4893e-01,  3.8305e-01, -4.6576e-01,  4.4450e-01,\n",
              "        -2.6261e-01,  3.7822e-01, -8.1114e-01,  5.8833e-01, -1.1221e+00,\n",
              "        -1.0772e+00,  9.9955e-01, -5.9661e-01, -5.3347e-01, -8.4481e-01,\n",
              "         3.5051e-01,  1.5749e+00, -7.1264e-01,  2.8093e-01,  1.3830e+00,\n",
              "        -2.2705e+00,  1.7659e+00,  7.1859e-01, -1.1220e+00,  1.2615e+00,\n",
              "        -2.4013e+00,  5.7346e-01,  2.2604e-01, -9.7081e-01,  1.4429e+00,\n",
              "        -6.7860e-01,  2.3254e+00,  3.4848e-01, -4.0214e-02,  6.6526e-01,\n",
              "         3.8010e-01,  5.4544e-01, -1.6401e+00, -5.1380e-01, -1.9761e+00,\n",
              "         4.9141e-01,  1.1587e+00, -1.8599e-01,  3.4694e-01, -8.7822e-01,\n",
              "        -6.1021e-01,  6.7483e-01,  3.3707e-02,  4.2378e-01,  1.4605e-01,\n",
              "        -6.9838e-01, -8.7712e-01,  3.6539e-01,  3.1948e-02, -7.7184e-01,\n",
              "         1.0851e+00,  2.5437e-01, -5.0821e-01, -9.2205e-01, -3.1365e-01,\n",
              "         6.0147e-01, -1.0975e+00,  7.4916e-02,  3.7577e-01, -2.9408e+00,\n",
              "         1.7680e+00, -1.3368e+00,  6.9864e-01,  6.9373e-01, -9.2728e-01,\n",
              "         4.0709e-01, -2.5216e-01,  1.3281e+00,  1.3762e-01,  1.0165e+00,\n",
              "         1.6819e-01, -1.0322e+00, -9.0089e-01,  1.0159e+00, -7.9535e-02,\n",
              "        -2.0896e-01, -1.0162e-01,  6.8308e-03,  3.7877e-02,  4.0355e-01,\n",
              "         7.9412e-01, -7.3561e-01, -1.2344e-01, -2.1965e-01,  3.9589e-01,\n",
              "         1.7809e+00,  7.8298e-03,  5.5870e-01,  1.8319e+00,  1.1238e+00,\n",
              "         3.0168e-01, -1.3303e+00, -3.0673e+00, -8.2754e-01,  4.2973e-01,\n",
              "        -1.3146e+00, -7.8831e-01, -4.4020e-01,  3.2068e-01,  4.8608e-02,\n",
              "        -1.8614e+00,  5.0366e-01,  1.2068e+00, -5.1871e-01,  3.3785e-01,\n",
              "         5.3243e-02,  4.4939e-01, -7.6921e-01,  4.9665e-01,  6.6624e-01,\n",
              "        -7.3207e-01, -5.6308e-01,  5.4847e-01, -7.3537e-01,  3.6440e-01,\n",
              "         4.3245e+00,  3.0325e-01,  3.0362e-01,  1.4229e+00, -1.9148e+01,\n",
              "         1.8014e-01, -1.4987e+00, -1.0227e-01, -2.3416e+00, -3.1699e+00,\n",
              "        -1.3557e+00,  7.2308e-01,  8.1261e-01, -1.3652e+00, -1.9268e-01,\n",
              "        -3.9581e-01, -4.9745e-02,  4.0124e-01,  3.3022e-01,  2.7822e-01,\n",
              "        -1.4674e+00, -1.5769e+00,  7.9728e-01, -2.3792e-01,  5.2051e-01,\n",
              "         1.7458e-01,  6.5063e-01,  6.4234e-01,  5.3568e-01, -6.4569e-01,\n",
              "         1.8805e-01, -1.3922e+00, -3.1487e-01, -7.1289e-01, -4.2557e-01,\n",
              "        -3.1453e-01,  4.6427e-01,  4.9946e-01, -3.7651e-01,  1.5266e+00,\n",
              "        -7.7736e-01, -1.5193e+00,  3.5855e-01,  5.8229e-01,  7.1364e-02,\n",
              "         8.6256e-01,  1.7128e-01, -7.1436e-01,  1.3179e-01, -2.7910e-02,\n",
              "         1.1846e+00,  2.0745e+00, -9.8349e-03, -6.3736e-01,  4.9146e+00,\n",
              "         9.6406e-02, -1.3652e-01,  5.7626e-01, -3.6063e-01, -7.4183e-01,\n",
              "        -4.2731e-01,  1.0218e+00,  8.5915e-01, -2.2046e-01,  5.9426e-01,\n",
              "        -7.2842e-01, -1.8240e-02, -5.3892e-01, -4.4574e-02,  1.1378e+00,\n",
              "        -3.3224e-01,  2.4333e-01,  7.1258e-01,  1.1859e+00, -2.6563e-01,\n",
              "        -2.9709e-01, -9.1610e-01, -3.9742e-01, -8.4957e-01, -1.0683e+00,\n",
              "        -1.0958e+00,  9.7805e-01,  8.4362e-02, -7.7760e-01,  4.7036e-01,\n",
              "        -5.2448e-01, -6.2180e+00, -4.9132e-02, -9.4179e-01, -2.0253e-01,\n",
              "        -2.4075e-01, -8.2754e-01,  1.0557e+00, -3.0916e-03,  6.6955e-01,\n",
              "        -1.1343e+00, -1.4231e-01,  1.6242e-01], device='cuda:0')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "activations[0, 0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Enm-R61Dg1b"
      },
      "source": [
        "Where to look for the \"positive sentiment\" representation in the big activation space? The localization of concepts in the intermediate layer outputs is an active area of research. Multiple findings suggest that the output of layers roughly halfway throughout the model layers contain most abstract semantic concepts (using linear probes, counterfactual interventions, ...). See [this post](https://sidn.baulab.info/stages/#the-remarkable-robustness-of-llms) on different \"stages\" in a Transformer forward-pass.\n",
        "\n",
        "The model diagram above showed that GPT-2 has 12 layers. Let's cache the intermediate activation of \"positive\" and \"negative\" at the output of the middle layers 4 and 5. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Looking back at the model diagram, we can directly access the layer modules by their index.\n",
        "layer_idxs = [4, 5]\n",
        "layer_modules = [model.transformer.h[idx] for idx in layer_idxs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead, of searching through many activations, we will take a set off each positive sentences and negative sentences. The mean activation vectors of each set might encode a general representation about positive/negative sentiment.\n",
        "\n",
        "How can we mitigate picking up on spurious, unwanted features when computing the steering vector from a set of positive/negative sentences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "POSITIVE_SENTENCES = [\n",
        "    \"The weather is really nice\",\n",
        "    \"I'm so happy\",\n",
        "    \"This cake is absolutely delicious\",\n",
        "    \"I love my friends\",\n",
        "    \"I'm feeling great\",\n",
        "    \"I'm so excited\",\n",
        "    \"This is the best day ever\",\n",
        "    \"I really like this gift\",\n",
        "    \"Croissants are my favorite\",\n",
        "    \"The movie was fantastic\",\n",
        "    \"I got a promotion at work\",\n",
        "    \"My vacation was amazing\",\n",
        "    \"The concert exceeded my expectations\",\n",
        "    \"I'm grateful for my family\",\n",
        "    \"This book is incredibly engaging\",\n",
        "    \"The restaurant service was excellent\",\n",
        "    \"I'm proud of my accomplishments\",\n",
        "    \"The sunset is breathtakingly beautiful\",\n",
        "    \"I passed my exam with flying colors\",\n",
        "    \"This coffee tastes perfect\",\n",
        "]\n",
        "\n",
        "NEGATIVE_SENTENCES = [\n",
        "    \"The weather is really bad\",\n",
        "    \"I'm so sad\",\n",
        "    \"This cake is completely inedible\",\n",
        "    \"I hate my enemies\",\n",
        "    \"I'm feeling awful\",\n",
        "    \"I'm so anxious\",\n",
        "    \"This is the worst day ever\",\n",
        "    \"I dislike this gift\",\n",
        "    \"Croissants are disgusting\",\n",
        "    \"The movie was terrible\",\n",
        "    \"I got fired from work\",\n",
        "    \"My vacation was a disaster\",\n",
        "    \"The concert was a huge disappointment\",\n",
        "    \"I'm frustrated with my family\",\n",
        "    \"This book is incredibly boring\",\n",
        "    \"The restaurant service was horrible\",\n",
        "    \"I'm ashamed of my mistakes\",\n",
        "    \"The weather is depressingly gloomy\",\n",
        "    \"I failed my exam miserably\",\n",
        "    \"This coffee tastes awful\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, lets cache the model activations for all sentences at layers 4 and 5 to compute the mean vectors we want to use for steering.\n",
        "\n",
        "How sensitive is steering to the layer selection?\n",
        "Does steering work better / worse for bigger models?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcXzSg9zDg1b",
        "outputId": "5b85a57f-159f-4e0f-fd27-4a82244214e9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_mean_activation(sentences, layer_outputs):\n",
        "    \"\"\"Compute mean activation over attended tokens for each layer.\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of sentences to compute mean activation over\n",
        "        layer_outputs: List of model layers to extract activations from\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping layers to their mean attended activations\n",
        "    \"\"\"\n",
        "    model.tokenizer.pad_token = model.tokenizer.eos_token\n",
        "    model.tokenizer.padding_side = \"left\"\n",
        "    tokens = model.tokenizer.batch_encode_plus(sentences, return_tensors=\"pt\", padding=True)\n",
        "    acts = {}\n",
        "    \n",
        "    with model.trace(tokens):\n",
        "        for layer in layer_outputs:\n",
        "            # Get activations from layer output tuple\n",
        "            activation = layer.output[0]\n",
        "            # Compute mean over attended tokens only\n",
        "            acts[layer] = activation.save()\n",
        "\n",
        "    # Compute mean over attended tokens only\n",
        "    mean_acts = {}\n",
        "    batch_arange = np.arange(len(sentences))[:, None]\n",
        "    for layer in layer_outputs:\n",
        "        mean_acts[layer] = acts[layer][batch_arange, tokens[\"attention_mask\"], :].mean(dim=(0,1))\n",
        "        # mean_acts[layer] = acts[layer][:, -1, :].mean(dim=0)\n",
        "    \n",
        "    return mean_acts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "positive_acts = compute_mean_activation(POSITIVE_SENTENCES, layer_modules)\n",
        "negative_acts = compute_mean_activation(NEGATIVE_SENTENCES, layer_modules)\n",
        "\n",
        "pos_neg_diff = {}\n",
        "for layer in layer_modules:\n",
        "    pos_neg_diff[layer] = positive_acts[layer] - negative_acts[layer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBC3v5OMDg1b"
      },
      "source": [
        "## Steering with activation addition\n",
        "\n",
        "Let's add this representation with a (cherry-picked) factor of 5 to the final token of our original prompt, where the prediction for the next token is made.\n",
        "\n",
        "Thanks to Andy Arditi for useful feedback. This experiment is inspired by Nina Panickssery's [steering demo](https://github.com/nrimsky/lmexp/blob/main/lmexp/notebooks/llama3.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def steer_model_output(model, prompt, layer_modules, activation_diff, steering_factor=1.0):\n",
        "    \"\"\"Generate text from prompt while steering activations using contrastive vectors.\n",
        "    \n",
        "    Args:\n",
        "        model: nnsight.LanguageModel instance\n",
        "        prompt: String prompt to generate from\n",
        "        act_diff: Dict mapping layer modules to their pos-neg activation differences\n",
        "        steering_factor: Float multiplier for steering strength (default: 1.0)\n",
        "        \n",
        "    Returns:\n",
        "        String containing the generated text\n",
        "    \"\"\"\n",
        "    with model.generate(prompt, max_new_tokens=20):\n",
        "        # Apply steering to each layer's activations\n",
        "        for layer in layer_modules:\n",
        "            out = layer.output  # Cache current activation tuple\n",
        "            acts = out[0]  # Get activation tensor\n",
        "            diff_vec = activation_diff[layer]  # Get steering vector for this layer\n",
        "            \n",
        "            # Add scaled steering vector to activations\n",
        "            # [None, None, :] broadcasts vector across batch and sequence dims\n",
        "            acts += steering_factor * diff_vec[None, None, :]\n",
        "            \n",
        "            # Update layer with modified activations\n",
        "            layer.output = (acts,) + out[1:]  # Preserve any other tuple elements\n",
        "        \n",
        "        # Save final output tokens\n",
        "        out_tokens = model.generator.output.save()\n",
        "    \n",
        "    # Decode tokens to text\n",
        "    out_text = model.tokenizer.batch_decode(out_tokens)[0]\n",
        "    return out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Manipulate the model with different steering factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"I think that this city is going to be a very different place in the future. I think that we're going to have a\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# No steering\n",
        "steer_model_output(model, prompt, layer_modules, pos_neg_diff, steering_factor=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I think that this city is a very good place to live. I think that we have a lot of good things going on here'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Positive steering\n",
        "steer_model_output(model, prompt, layer_modules, pos_neg_diff, steering_factor=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'I think that this city is not going to be able to compete with the other cities in the country,\" said the mayor.\\n'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Negative steering\n",
        "steer_model_output(model, prompt, layer_modules, pos_neg_diff, steering_factor=-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Follow-up questions\n",
        "\n",
        "This is a cherry-picked example. Do our findings generalize?\n",
        "\n",
        "Here are a few questions to play around with the code above.\n",
        "\n",
        "- Does the steering factor proportionally modulate the enthusiasm?\n",
        "- Are there limits to the steering factor?\n",
        "- Is the steering behavior dependent on the dataset (POSITIVE_SENTENCES, NEGATIVE_SENTENCES)\n",
        "- Does the contrastive steering method work for other binary datasets (eg. angry / calm) or other concepts like \"Speaking in the thick dialect of a pirate\"?\n",
        "- Does the token position of where steering vectors are extracted matter?\n",
        "- Does the token position of where steering vectors are inserted matter?\n",
        "\n",
        "- If steering doesn't work, is it because of the steering method or because the model is not aware of this concept in the first place?\n",
        "- Can we steer model knowledge? (Knowledge editing is its own subfield in interpretability, start by reading the ROME and MEMIT papers from David Bau's lab)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqsJ8xKGDg1c"
      },
      "source": [
        "## Other Demos\n",
        "\n",
        "- The [Contrastive Activation Addition paper](https://arxiv.org/abs/2312.06681) by Nina Panickssery discusses steering techniques in depth. Their demo notebooks are great and inspired this notebook:\n",
        "    - [Demo with GPT-2](https://github.com/nrimsky/lmexp/blob/main/lmexp/notebooks/gpt2small.ipynb) (very small model we used in this tutorial)\n",
        "    - [Demo with Llama3-chat](https://github.com/nrimsky/lmexp/blob/main/lmexp/notebooks/llama3.ipynb) (newer, smarter, bigger model than GPT-2, tuned to have chat-like conversations. Note this model is still very small compared to today's frontier models like Claude and models that power Chat-GPT).\n",
        "- The [Steering Tutorial in the ARENA program by Callum McDougall](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4.2]_Function_Vectors_&_Model_Steering) focuses on steering functional representations\n",
        "- The [Steering Tutorial by Decode Research](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb) demonstrates steering using sparse autoencoders, the same technique used in [Neuronpedia Steer](https://www.neuronpedia.org/gemma-2-9b-it/steer)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "steering_38c3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32bb365d9b354aaa8ed707ad548390dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475e5578ed054e95ac9347711cb743d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485bbefdaf1a4bfe97b0b5a179db1771": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b64092469c846edb57c8647356b8fa1",
              "IPY_MODEL_6170221a404e4bdfb66b17964de89c46",
              "IPY_MODEL_a77a3a3d9fe541b5b16a67b51d7f55fa"
            ],
            "layout": "IPY_MODEL_b3800c1c3c21466fbb3865c98cdb55c8"
          }
        },
        "4ccf54d063774af68bbbaee014e25c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b64092469c846edb57c8647356b8fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475e5578ed054e95ac9347711cb743d5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_615693fe0d06463d8041b571079ca4aa",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "615693fe0d06463d8041b571079ca4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6170221a404e4bdfb66b17964de89c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_900e7b20afb54dfc97dc1ab3a71894f1",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa1b82448144c3ebc01e3d058adb9f2",
            "value": 124
          }
        },
        "6fa1b82448144c3ebc01e3d058adb9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "900e7b20afb54dfc97dc1ab3a71894f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a77a3a3d9fe541b5b16a67b51d7f55fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32bb365d9b354aaa8ed707ad548390dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ccf54d063774af68bbbaee014e25c91",
            "value": "â€‡124/124â€‡[00:00&lt;00:00,â€‡5.18kB/s]"
          }
        },
        "b3800c1c3c21466fbb3865c98cdb55c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
