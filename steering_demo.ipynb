{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7rvLDsGDg1Y"
      },
      "source": [
        "## ðŸ•¹ï¸ Hack your LLM: Modify chatbot behavior with activation steering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers nnsight==0.4.0.dev\n",
        "\n",
        "from nnsight import LanguageModel\n",
        "model_nn = LanguageModel(\"openai-community/gpt2-xl\", device_map=\"cuda:0\", dispatch=True) # takes about 3 minutes on colab"
      ],
      "metadata": {
        "id": "JlIoCoC5DoRv",
        "outputId": "ced6f1d4-4b85-4be9-e6ff-dc43a8966966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998,
          "referenced_widgets": [
            "485bbefdaf1a4bfe97b0b5a179db1771",
            "5b64092469c846edb57c8647356b8fa1",
            "6170221a404e4bdfb66b17964de89c46",
            "a77a3a3d9fe541b5b16a67b51d7f55fa",
            "b3800c1c3c21466fbb3865c98cdb55c8",
            "475e5578ed054e95ac9347711cb743d5",
            "615693fe0d06463d8041b571079ca4aa",
            "900e7b20afb54dfc97dc1ab3a71894f1",
            "6fa1b82448144c3ebc01e3d058adb9f2",
            "32bb365d9b354aaa8ed707ad548390dc",
            "4ccf54d063774af68bbbaee014e25c91"
          ]
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: nnsight==0.4.0.dev in /usr/local/lib/python3.10/dist-packages (0.4.0.dev0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (4.25.5)\n",
            "Requirement already satisfied: python-socketio[client] in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (5.12.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.21.0)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (2.10.3)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (2.5.1+cu121)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.20.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (1.2.1)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.31.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.8.0)\n",
            "Requirement already satisfied: msgspec in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.19.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from nnsight==0.4.0.dev) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight==0.4.0.dev) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.9.0->nnsight==0.4.0.dev) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight==0.4.0.dev) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight==0.4.0.dev) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->nnsight==0.4.0.dev) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->nnsight==0.4.0.dev) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->nnsight==0.4.0.dev) (5.9.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight==0.4.0.dev) (8.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers->nnsight==0.4.0.dev) (11.0.0)\n",
            "Requirement already satisfied: bidict>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight==0.4.0.dev) (0.23.1)\n",
            "Requirement already satisfied: python-engineio>=4.11.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight==0.4.0.dev) (4.11.2)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from python-socketio[client]->nnsight==0.4.0.dev) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: simple-websocket>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from python-engineio>=4.11.0->python-socketio[client]->nnsight==0.4.0.dev) (1.1.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers->nnsight==0.4.0.dev) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->nnsight==0.4.0.dev) (3.0.2)\n",
            "Requirement already satisfied: wsproto in /usr/local/lib/python3.10/dist-packages (from simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight==0.4.0.dev) (1.2.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio>=4.11.0->python-socketio[client]->nnsight==0.4.0.dev) (0.14.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "485bbefdaf1a4bfe97b0b5a179db1771"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MC1DVJYnHzea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4kRr0KwDg1Z"
      },
      "source": [
        "## Prompt a model\n",
        "\n",
        "Huggingface is the main platform for open-weight models. Here's a simple example of how to load and prompt the GPT-2 model by OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SWFnZpLDg1Z",
        "outputId": "a3b24041-682d-4a67-e4c9-48d7ffe73cfe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/canrager/.pyenv/versions/3.11.9/envs/steering_38c3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZD2JBd5Dg1Z"
      },
      "source": [
        "First, convert the prompt string to a list of tokens, the input format for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EPq1Zj2Dg1a",
        "outputId": "228f1fa2-660b-4633-a58c-f93cc2bd802f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  464, 26175, 32650,   373, 15646,   287,   262,  1748,   286]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"The Hamburger was invented in the city of\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs.input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WXlhJ4dDg1a",
        "outputId": "816b355c-ed22-4d61-a308-d5ddacbe930e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The', 'Ä Hamb', 'urger', 'Ä was', 'Ä invented', 'Ä in', 'Ä the', 'Ä city', 'Ä of']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Visualize what individual tokens mean\n",
        "tokenizer.convert_ids_to_tokens(inputs.input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFE3zhXODg1a"
      },
      "source": [
        "The \"Ä \" is treated as a leading space. Let's generate 100 tokens follwing our prompt with the GPT2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCtUmOkbDg1a",
        "outputId": "cb148d2b-5370-489f-e878-d026c7e1313b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"The Hamburger was invented in the city of Hamburg by the late Karl G. Berlich, a German chemist. The Hamburger's inventor has had several family members and associates involved in the Hamburger movement.\\n\\nKarl G. Berlich,\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_tokens = model.generate(**inputs, do_sample=True, max_length=50)\n",
        "\n",
        "# The generated tokens are indices that need to be converted to text\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "gen_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXh2DTy1Dg1a"
      },
      "source": [
        "## Prompt injection\n",
        "\n",
        "Let's simply ask the model to answer in the style of a pirate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESiRoHZIDg1a",
        "outputId": "0ebee070-b7a6-47ce-e07c-e05ae7d58d79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'You are captain Blackbeard who just returned from a long adventure. Speak in a thick dialect. The Hamburger was invented in the city of New York (1816). The character is considered a character of high character and the name is derived from the'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_injection = \"You are captain Blackbeard who just returned from a long adventure. Speak in a thick dialect. \"\n",
        "\n",
        "prompt_inj = prompt_injection + prompt\n",
        "inputs_inj = tokenizer(prompt_inj, return_tensors=\"pt\")\n",
        "gen_tokens_inj = model.generate(**inputs_inj, do_sample=True, max_length=50)\n",
        "gen_text_inj = tokenizer.batch_decode(gen_tokens_inj)[0]\n",
        "gen_text_inj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikFutuHfDg1a"
      },
      "source": [
        "Hmm. The pirate-style of this message can be better. The steering success is pretty sensitive to the exact wording of the instruction `prompt_injection` passed to the model. We could go down the rabbit hole of prompt engineering at this point. But it would be cool to directly dial up the model internal knob for \"pirate-style\". Luckily, we have full access to the model weigths!\n",
        "\n",
        "Disclamer: There's no guarantee whether this knob exists at all. But recent work in language model interpretability found that many semantic concepts are linearly encoded in activation space ([Park et al.](https://arxiv.org/abs/2311.03658) summarize findings well). Next, we'll try to find a linear \"pirate-direction\" in activation space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtWugcmLDg1a"
      },
      "source": [
        "## Accessing model internals\n",
        "\n",
        "Model inference is a seqence of matrix operations. Let's have a look at the layer structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKEwubL1Dg1b",
        "outputId": "f42df99e-001f-4db8-931c-38ce4fa5d00e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRBAgTmaDg1b"
      },
      "source": [
        "### Transformer Explainers\n",
        "\n",
        "The neural network architecture of GPT-2 is called a decoder-only Transformer. Callum McDougall created [my favourite explainer of the Transformer architecture](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch). Another popular ressource is [Jay Alammar's blogpost](https://jalammar.github.io/illustrated-gpt2/). Anthropic's [Mathematical Framework of Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html) provides deeper conceptual understanding of the transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hynPWboDg1b"
      },
      "source": [
        "### Activation Caching\n",
        "\n",
        "We'll use the `nnsight` library to access the intermediate results of those matrix opertations. The `nnsight.LanugageModel` class is a wrapper around the `transformers.AutoModelForCausalLM` class we loaded above. Generating text goes like this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nn"
      ],
      "metadata": {
        "id": "ZlAYt4gBI3k2",
        "outputId": "1e3f37bb-7db0-4f6f-c839-339b904d9638",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1600)\n",
              "    (wpe): Embedding(1024, 1600)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-47): 48 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
              "  (generator): Generator(\n",
              "    (streamer): Streamer()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnx49x0BDg1b"
      },
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel\n",
        "model_nn = LanguageModel(\"openai-community/gpt2-xl\", device_map=\"cuda\", dispatch=True) # takes about 3 minutes on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "mXKoo_4pDg1b",
        "outputId": "923264f4-0a4e-48bd-e61f-e97cfd8b631f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I went up to my friend and said, 'I'm going to go to the bathroom.' I was like, 'I'm going to go to the bathroom.' I was like, 'I'm going to go to the bathroom.' I was like, 'I'm going to go to\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "prompt = \"I went up to my friend and said\"\n",
        "\n",
        "with model_nn.generate(prompt, max_new_tokens=50): # The nnsight also takes the prompt string as input and does the tokenization internally\n",
        "    out_tokens = model_nn.generator.output.save()\n",
        "\n",
        "out_text = model_nn.tokenizer.batch_decode(out_tokens)[0]\n",
        "out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Enm-R61Dg1b"
      },
      "source": [
        "Where to look for the pirate representation? The localization of concepts in the intermediate layer outputs is an active area of research. Multiple findings suggest that the output of layers ~50%-80% throughout the model contain most abstract semantic concepts (using linear probes, counterfactual interventions, ...). See [this post](https://sidn.baulab.info/stages/#the-remarkable-robustness-of-llms) on different \"stages\" in a Transformer forward-pass.\n",
        "\n",
        "GPT-2 has 12 layers, let's cache the intermediate activation of \"happy\" and \"sad\" at the output of layer 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcXzSg9zDg1b",
        "outputId": "5b85a57f-159f-4e0f-fd27-4a82244214e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 1600])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "layer_8 = model_nn.transformer.h[6] # You can find the name of the layer module in the model diagram above\n",
        "\n",
        "with model_nn.trace(\"I love talking about weddings\"): # NOTE 1: Trace is a single forward pass, no interative, auto-regressive generation.\n",
        "    happy_activation = layer_8.output[0].save() # Confusingly layer_8.output returns a tuple, the activations we want are at idx 0\n",
        "\n",
        "with model_nn.trace(\"I hate talking about weddings\"):\n",
        "    sad_activation = layer_8.output[0].save()\n",
        "\n",
        "happy_activation.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr7CL5L4Dg1b"
      },
      "source": [
        "The model internal activations are of shape `[prompt_in_the_batch, token_position, model_dimension]`. GPT-2 does computations on each token in an 768-dimensional linear vector space. The final pirate token representation at layer 8 looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooTRElE_Dg1b",
        "outputId": "d3e89460-e972-405e-afcd-65399b61022e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  1.2969,  -2.5545,  -1.0699,  ..., -14.4872,   0.1708,  -1.3836],\n",
              "         [ -2.0083,  -1.0310,  -4.0910,  ...,  -4.5689,   0.9379,  -2.7911],\n",
              "         [  2.9459,  -4.1832,   0.4191,  ...,  -1.8145,   0.0177,  -0.0807],\n",
              "         [ -0.5364,  -0.9128,  -7.7111,  ...,  -3.6645,  -2.0546,   0.1507],\n",
              "         [ -4.9883,  -7.1793,  -2.3995,  ...,  -6.0699,   0.6344,   0.0563]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "happy_activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBC3v5OMDg1b"
      },
      "source": [
        "## Steering with activation addition\n",
        "\n",
        "Let's add this representation with an (arbitrarily chosen) factor of 2 to the final token of our original prompt, where the prediction for the next token is made."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_idx = 6\n",
        "\n",
        "layer_8 = model_nn.transformer.h[layer_idx] # You can find the name of the layer module in the model diagram above\n",
        "\n",
        "with model_nn.trace(\"I love talking about weddings\"): # NOTE 1: Trace is a single forward pass, no interative, auto-regressive generation.\n",
        "    happy_activation = layer_8.output[0].save() # Confusingly layer_8.output returns a tuple, the activations we want are at idx 0\n",
        "\n",
        "with model_nn.trace(\"I hate talking about weddings\"):\n",
        "    sad_activation = layer_8.output[0].save()\n",
        "\n",
        "happy_activation.shape"
      ],
      "metadata": {
        "id": "Ja08KR20Kw8v",
        "outputId": "17e62661-01cc-4721-8f0d-92ca3a084ffc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 1600])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5JiJJbNuDg1b"
      },
      "outputs": [],
      "source": [
        "# We'll be steering with contrastive activation addition\n",
        "act_diff = happy_activation[0, -1, :] - sad_activation[0, -1, :]\n",
        "steering_factor = 10\n",
        "steering_vector = steering_factor * act_diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6lRc1hXDg1b",
        "outputId": "c865ce26-f46e-46e5-cf7f-53d76b6fe834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/graph/node.py\", line 289, in execute\n",
            "    self.target.execute(self)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/contexts/interleaving.py\", line 161, in execute\n",
            "    graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/mixins/meta.py\", line 47, in interleave\n",
            "    return super().interleave(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/base.py\", line 333, in interleave\n",
            "    with interleaver:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/interleaver.py\", line 125, in __exit__\n",
            "    raise exc_val\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/base.py\", line 334, in interleave\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/modeling/language.py\", line 305, in _generate\n",
            "    output = self._model.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2252, in generate\n",
            "    result = self._sample(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3251, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
            "    return inner()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1272, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1844, in _call_impl\n",
            "    return inner()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1790, in inner\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\", line 1146, in forward\n",
            "    presents = presents + (outputs[1],)\n",
            "IndexError: tuple index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n",
            "    lambda f: self._run_callback(functools.partial(callback, future))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n",
            "    ret = callback()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n",
            "    self.ctx_run(self.run)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n",
            "    yielded = self.gen.send(value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
            "    yield gen.maybe_future(dispatch(*args))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n",
            "    yield gen.maybe_future(handler(stream, idents, msg))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n",
            "    self.do_execute(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n",
            "    yielded = ctx_run(next, result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n",
            "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n",
            "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-47-54dd36fe604d>\", line 3, in <cell line: 3>\n",
            "    with model_nn.generate(prompt, max_new_tokens=50):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/contexts/interleaving.py\", line 96, in __exit__\n",
            "    super().__exit__(exc_type, exc_val, exc_tb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/contexts/tracer.py\", line 25, in __exit__\n",
            "    return super().__exit__(exc_type, exc_val, exc_tb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/contexts/base.py\", line 74, in __exit__\n",
            "    self.add(graph.stack[-1], graph, *self.args, **self.kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/protocols/base.py\", line 17, in add\n",
            "    return graph.create(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/graph/graph.py\", line 131, in create\n",
            "    return self.proxy_class(self.node_class(target, *args, graph=graph, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/intervention/graph/node.py\", line 30, in __init__\n",
            "    super().__init__(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/graph/node.py\", line 63, in __init__\n",
            "    self.meta_data = self._meta_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/graph/node.py\", line 404, in _meta_data\n",
            "    meta_data[\"traceback\"] = traceback_str()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nnsight/tracing/graph/node.py\", line 392, in traceback_str\n",
            "    stack = traceback.extract_stack()\n",
            "\n",
            "nnsight.util.NNsightError: tuple index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: 'NNsightError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AssertionError\n",
            "nnsight.util.NNsightError: tuple index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: 'NNsightError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AssertionError\n",
            "nnsight.util.NNsightError: tuple index out of range\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: 'NNsightError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "AssertionError\n"
          ]
        }
      ],
      "source": [
        "num_generated_tokens = 50\n",
        "\n",
        "with model_nn.generate(prompt, max_new_tokens=50):\n",
        "    layer_8 = model_nn.transformer.h[layer_idx] # Reinitialize the layer object\n",
        "\n",
        "    layer_8_out = layer_8.output # Cache the current activaiton, tuple\n",
        "    layer_8_acts = layer_8_out[0]\n",
        "    layer_8_acts[:, 0] += steering_vector # Modify\n",
        "    layer_8.output = (layer_8_acts,)# + layer_8_out[1:] # Update the layer with the modified activations\n",
        "\n",
        "    out_tokens = model_nn.generator.output.save()\n",
        "\n",
        "out_text = model_nn.tokenizer.batch_decode(out_tokens)\n",
        "out_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSffBWPADg1c"
      },
      "source": [
        "## Quickstart\n",
        "\n",
        "[Neuronpedia](https://www.neuronpedia.org/gemma-2-9b-it/steer)\n",
        "\n",
        "[Transluce Monitor](https://monitor.transluce.org/dashboard/chat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqsJ8xKGDg1c"
      },
      "source": [
        "## Other Demos\n",
        "\n",
        "- [Steering Tutorial in the ARENA program by Callum McDougall](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4.2]_Function_Vectors_&_Model_Steering)\n",
        "- [Steering Tutorial SAELens by Decode Research](https://github.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "485bbefdaf1a4bfe97b0b5a179db1771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b64092469c846edb57c8647356b8fa1",
              "IPY_MODEL_6170221a404e4bdfb66b17964de89c46",
              "IPY_MODEL_a77a3a3d9fe541b5b16a67b51d7f55fa"
            ],
            "layout": "IPY_MODEL_b3800c1c3c21466fbb3865c98cdb55c8"
          }
        },
        "5b64092469c846edb57c8647356b8fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475e5578ed054e95ac9347711cb743d5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_615693fe0d06463d8041b571079ca4aa",
            "value": "generation_config.json:â€‡100%"
          }
        },
        "6170221a404e4bdfb66b17964de89c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_900e7b20afb54dfc97dc1ab3a71894f1",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6fa1b82448144c3ebc01e3d058adb9f2",
            "value": 124
          }
        },
        "a77a3a3d9fe541b5b16a67b51d7f55fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32bb365d9b354aaa8ed707ad548390dc",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ccf54d063774af68bbbaee014e25c91",
            "value": "â€‡124/124â€‡[00:00&lt;00:00,â€‡5.18kB/s]"
          }
        },
        "b3800c1c3c21466fbb3865c98cdb55c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475e5578ed054e95ac9347711cb743d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615693fe0d06463d8041b571079ca4aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "900e7b20afb54dfc97dc1ab3a71894f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fa1b82448144c3ebc01e3d058adb9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32bb365d9b354aaa8ed707ad548390dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ccf54d063774af68bbbaee014e25c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}